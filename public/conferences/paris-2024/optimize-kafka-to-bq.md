# Optimizing Data Transfer Kafka to BQ: let's use Scala to make it custom

- Kind: Talk
- Slug: optimize-data-transfer-kafka-to-bq
- Category: Data
- confirmed: true

## Abstract

```
I will explore how our team developed a custom solution to address the escalating data and cost challenges of traditional cloud services in our data pipeline. Initially, we used Dataflow to move 2 billion daily events from Kafka to BigQuery, but rising costs prompted a shift to a more sustainable model. I will describe our transition to a two-part custom application that stages data in GCP cloud storage before loading it into BigQuery, and highlight how Scalaâ€™s features and the ZIO ecosystem contributed significantly to the performance and reliability of this application.
```

## Speakers

### Dario Amorosi

- photoRelPath: /images/profiles/paris-2024/dAmorosi.webp
- job: Big Data Engineer @ Adevinta

#### Links

- [Linkedin](https://www.linkedin.com/in/dario-amorosi-019317151)

#### Bio

```
I am a Big Data Engineer with extensive experience in building large-scale data systems and streaming architectures. Currently at Adevinta, in a central team responsible for building custom solutions for the company's data platform. With a background in computer engineering, I have been part of projects in different domains ranging from energy to space technology. I found in Scala an elegant way to express concepts quickly and efficiently, which improved my daily programming life.
```
