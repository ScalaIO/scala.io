# Running LLMs Locally with Scala: From APIs to Local Inference

- Kind: Talk
- Slug: running-llms-locally-with-scala
- Category: AI
- confirmed: true

## Abstract

```
Large Language Models (LLMs) are often seen as remote cloud-only services. What if you could run them locally, on your own machine, with full control over performance, privacy, and cost? In this talk, we’ll explore the ecosystem of local LLM runtimes such as vLLM, Ollama, and llama.cpp, and show how Scala developers can interact with these models through the OpenAI-compatible API. You’ll see how to set up a client in Scala, send prompts, manage streaming completions, and even build simple AI-assisted applications, without relying on external hosted APIs. The session will include live demos of running and querying local models directly from Scala code, plus a discussion of trade-offs: performance vs. hardware requirements, model quality vs. size, and developer experience vs. operational complexity. We’ll also explore how these tools integrate with the wider Scala ecosystem, opening the door to privacy-preserving AI applications, offline prototyping, and hybrid cloud/local deployments.
```

## Speakers

### Sören Dreano

- photoRelPath: /images/profiles/paris-2025/sDreano.webp
- job: Machine Learning Scientist and Engineer @ NuMind

#### Links

#### Bio

```
I’m a Machine Learning Scientist and Engineer at NuMind, where I focus on deploying large language models in production and optimizing inference speed. My academic background is in Natural Language Processing, with a PhD on Transformer architectures. Although I began my journey as a Python and PyTorch specialist, Scala has quickly won me over with its strong type system and functional programming power, especially when building reliable, production-ready systems. Outside of research and engineering, I’m passionate about bridging the gap between cutting-edge AI and practical developer workflows, making advanced tools accessible to more people.
```
